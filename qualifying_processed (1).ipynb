{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cadeb1ac-1004-4979-be42-2bfad9d85dc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[1]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.mkdirs(\"dbfs:/FileStore/Formula1/qualifying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8024c28d-5701-4eb0-8524-f36ef039a707",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Step 1 - Read the JSON file using the spark dataframe reader API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dc3e1f2-fed8-4fc9-993e-9862c9801c72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e60cc1a-501b-423f-9787-605c40bc00cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "qualifying_schema = StructType(fields=[StructField(\"qualifyId\", IntegerType(), False),\n",
    "                                      StructField(\"raceId\", IntegerType(), True),\n",
    "                                      StructField(\"driverId\", IntegerType(), True),\n",
    "                                      StructField(\"constructorId\", IntegerType(), True),\n",
    "                                      StructField(\"number\", IntegerType(), True),\n",
    "                                      StructField(\"position\", IntegerType(), True),\n",
    "                                      StructField(\"q1\", StringType(), True),\n",
    "                                      StructField(\"q2\", StringType(), True),\n",
    "                                      StructField(\"q3\", StringType(), True),\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bf29f66-c67a-4895-a875-5cdab4b8758b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "qualifying_df = spark.read.json(\"dbfs:/FileStore/Formula1/qualifying\",schema =qualifying_schema , multiLine= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0eb7e3f-aa0f-46c0-aced-821b387bc787",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+--------+-------------+------+--------+--------+--------+--------+\n|qualifyId|raceId|driverId|constructorId|number|position|      q1|      q2|      q3|\n+---------+------+--------+-------------+------+--------+--------+--------+--------+\n|        1|    18|       1|            1|    22|       1|1:26.572|1:25.187|1:26.714|\n|        2|    18|       9|            2|     4|       2|1:26.103|1:25.315|1:26.869|\n|        3|    18|       5|            1|    23|       3|1:25.664|1:25.452|1:27.079|\n|        4|    18|      13|            6|     2|       4|1:25.994|1:25.691|1:27.178|\n|        5|    18|       2|            2|     3|       5|1:25.960|1:25.518|1:27.236|\n|        6|    18|      15|            7|    11|       6|1:26.427|1:26.101|1:28.527|\n|        7|    18|       3|            3|     7|       7|1:26.295|1:26.059|1:28.687|\n|        8|    18|      14|            9|     9|       8|1:26.381|1:26.063|1:29.041|\n|        9|    18|      10|            7|    12|       9|1:26.919|1:26.164|1:29.593|\n|       10|    18|      20|            5|    15|      10|1:26.702|1:25.842|      \\N|\n|       11|    18|      22|           11|    17|      11|1:26.369|1:26.173|      \\N|\n|       12|    18|       4|            4|     5|      12|1:26.907|1:26.188|      \\N|\n|       13|    18|      18|           11|    16|      13|1:26.712|1:26.259|      \\N|\n|       14|    18|       6|            3|     8|      14|1:26.891|1:26.413|      \\N|\n|       15|    18|      17|            9|    10|      15|1:26.914|      \\N|      \\N|\n|       16|    18|       8|            6|     1|      16|1:26.140|      \\N|      \\N|\n|       17|    18|      21|           10|    21|      17|1:27.207|      \\N|      \\N|\n|       18|    18|       7|            5|    14|      18|1:27.446|      \\N|      \\N|\n|       19|    18|      16|           10|    20|      19|1:27.859|      \\N|      \\N|\n|       20|    18|      11|            8|    18|      20|1:28.208|      \\N|      \\N|\n+---------+------+--------+-------------+------+--------+--------+--------+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "qualifying_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adfa7e58-c735-4635-b673-01af2d359483",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Step 2 - Rename columns and add new columns\n",
    "#Rename qualifyingId, driverId, constructorId and raceId\n",
    "#Add ingestion_date with current timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f2862b0-af06-4909-802a-f4202a9828ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit , current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "713af3e0-c571-4ed9-adaa-dbc1282aefea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = qualifying_df.withColumnRenamed(\"qualifyId\", \"qualify_id\") \\\n",
    ".withColumnRenamed(\"driverId\", \"driver_id\") \\\n",
    ".withColumnRenamed(\"raceId\", \"race_id\") \\\n",
    ".withColumnRenamed(\"constructorId\", \"constructor_id\") \\\n",
    ".withColumn(\"ingestion_date\", current_timestamp()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "234bb229-72cc-4d97-a323-17b220d886b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------+--------------+------+--------+--------+--------+--------+-----------------------+\n|qualify_id|race_id|driver_id|constructor_id|number|position|q1      |q2      |q3      |ingestion_date         |\n+----------+-------+---------+--------------+------+--------+--------+--------+--------+-----------------------+\n|1         |18     |1        |1             |22    |1       |1:26.572|1:25.187|1:26.714|2024-04-23 04:00:32.158|\n|2         |18     |9        |2             |4     |2       |1:26.103|1:25.315|1:26.869|2024-04-23 04:00:32.158|\n|3         |18     |5        |1             |23    |3       |1:25.664|1:25.452|1:27.079|2024-04-23 04:00:32.158|\n|4         |18     |13       |6             |2     |4       |1:25.994|1:25.691|1:27.178|2024-04-23 04:00:32.158|\n|5         |18     |2        |2             |3     |5       |1:25.960|1:25.518|1:27.236|2024-04-23 04:00:32.158|\n|6         |18     |15       |7             |11    |6       |1:26.427|1:26.101|1:28.527|2024-04-23 04:00:32.158|\n|7         |18     |3        |3             |7     |7       |1:26.295|1:26.059|1:28.687|2024-04-23 04:00:32.158|\n|8         |18     |14       |9             |9     |8       |1:26.381|1:26.063|1:29.041|2024-04-23 04:00:32.158|\n|9         |18     |10       |7             |12    |9       |1:26.919|1:26.164|1:29.593|2024-04-23 04:00:32.158|\n|10        |18     |20       |5             |15    |10      |1:26.702|1:25.842|\\N      |2024-04-23 04:00:32.158|\n|11        |18     |22       |11            |17    |11      |1:26.369|1:26.173|\\N      |2024-04-23 04:00:32.158|\n|12        |18     |4        |4             |5     |12      |1:26.907|1:26.188|\\N      |2024-04-23 04:00:32.158|\n|13        |18     |18       |11            |16    |13      |1:26.712|1:26.259|\\N      |2024-04-23 04:00:32.158|\n|14        |18     |6        |3             |8     |14      |1:26.891|1:26.413|\\N      |2024-04-23 04:00:32.158|\n|15        |18     |17       |9             |10    |15      |1:26.914|\\N      |\\N      |2024-04-23 04:00:32.158|\n|16        |18     |8        |6             |1     |16      |1:26.140|\\N      |\\N      |2024-04-23 04:00:32.158|\n|17        |18     |21       |10            |21    |17      |1:27.207|\\N      |\\N      |2024-04-23 04:00:32.158|\n|18        |18     |7        |5             |14    |18      |1:27.446|\\N      |\\N      |2024-04-23 04:00:32.158|\n|19        |18     |16       |10            |20    |19      |1:27.859|\\N      |\\N      |2024-04-23 04:00:32.158|\n|20        |18     |11       |8             |18    |20      |1:28.208|\\N      |\\N      |2024-04-23 04:00:32.158|\n+----------+-------+---------+--------------+------+--------+--------+--------+--------+-----------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "final_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86fe4b98-ba85-4940-ad39-a0170ead3d3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Step 3 - Write to output to processed container in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa861aa1-c04f-41b5-bc6b-9ff9901cef8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3417363049632157>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mfinal_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdbfs:/FileStore/Formula1/processed/f1_processed.qualifying\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1655\u001B[0m, in \u001B[0;36mDataFrameWriter.parquet\u001B[0;34m(self, path, mode, partitionBy, compression)\u001B[0m\n",
       "\u001B[1;32m   1653\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpartitionBy(partitionBy)\n",
       "\u001B[1;32m   1654\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(compression\u001B[38;5;241m=\u001B[39mcompression)\n",
       "\u001B[0;32m-> 1655\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Path dbfs:/FileStore/Formula1/processed/f1_processed.qualifying already exists."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-3417363049632157>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mfinal_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdbfs:/FileStore/Formula1/processed/f1_processed.qualifying\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1655\u001B[0m, in \u001B[0;36mDataFrameWriter.parquet\u001B[0;34m(self, path, mode, partitionBy, compression)\u001B[0m\n\u001B[1;32m   1653\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpartitionBy(partitionBy)\n\u001B[1;32m   1654\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(compression\u001B[38;5;241m=\u001B[39mcompression)\n\u001B[0;32m-> 1655\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Path dbfs:/FileStore/Formula1/processed/f1_processed.qualifying already exists.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Path dbfs:/FileStore/Formula1/processed/f1_processed.qualifying already exists.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_df.write.parquet(\"dbfs:/FileStore/Formula1/processed/f1_processed.qualifying\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e5fc468-b42e-4105-8969-04bd4910a3f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------+--------------+------+--------+--------+--------+--------+-----------------------+\n|qualify_id|race_id|driver_id|constructor_id|number|position|q1      |q2      |q3      |ingestion_date         |\n+----------+-------+---------+--------------+------+--------+--------+--------+--------+-----------------------+\n|1         |18     |1        |1             |22    |1       |1:26.572|1:25.187|1:26.714|2024-04-23 04:01:10.743|\n|2         |18     |9        |2             |4     |2       |1:26.103|1:25.315|1:26.869|2024-04-23 04:01:10.743|\n|3         |18     |5        |1             |23    |3       |1:25.664|1:25.452|1:27.079|2024-04-23 04:01:10.743|\n|4         |18     |13       |6             |2     |4       |1:25.994|1:25.691|1:27.178|2024-04-23 04:01:10.743|\n|5         |18     |2        |2             |3     |5       |1:25.960|1:25.518|1:27.236|2024-04-23 04:01:10.743|\n|6         |18     |15       |7             |11    |6       |1:26.427|1:26.101|1:28.527|2024-04-23 04:01:10.743|\n|7         |18     |3        |3             |7     |7       |1:26.295|1:26.059|1:28.687|2024-04-23 04:01:10.743|\n|8         |18     |14       |9             |9     |8       |1:26.381|1:26.063|1:29.041|2024-04-23 04:01:10.743|\n|9         |18     |10       |7             |12    |9       |1:26.919|1:26.164|1:29.593|2024-04-23 04:01:10.743|\n|10        |18     |20       |5             |15    |10      |1:26.702|1:25.842|\\N      |2024-04-23 04:01:10.743|\n|11        |18     |22       |11            |17    |11      |1:26.369|1:26.173|\\N      |2024-04-23 04:01:10.743|\n|12        |18     |4        |4             |5     |12      |1:26.907|1:26.188|\\N      |2024-04-23 04:01:10.743|\n|13        |18     |18       |11            |16    |13      |1:26.712|1:26.259|\\N      |2024-04-23 04:01:10.743|\n|14        |18     |6        |3             |8     |14      |1:26.891|1:26.413|\\N      |2024-04-23 04:01:10.743|\n|15        |18     |17       |9             |10    |15      |1:26.914|\\N      |\\N      |2024-04-23 04:01:10.743|\n|16        |18     |8        |6             |1     |16      |1:26.140|\\N      |\\N      |2024-04-23 04:01:10.743|\n|17        |18     |21       |10            |21    |17      |1:27.207|\\N      |\\N      |2024-04-23 04:01:10.743|\n|18        |18     |7        |5             |14    |18      |1:27.446|\\N      |\\N      |2024-04-23 04:01:10.743|\n|19        |18     |16       |10            |20    |19      |1:27.859|\\N      |\\N      |2024-04-23 04:01:10.743|\n|20        |18     |11       |8             |18    |20      |1:28.208|\\N      |\\N      |2024-04-23 04:01:10.743|\n+----------+-------+---------+--------------+------+--------+--------+--------+--------+-----------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "final_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c62b99ef-12b5-472e-81e2-d8f9977820e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "qualifying_processed",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
