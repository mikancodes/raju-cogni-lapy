{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1084cda-61d8-4341-9f9a-880c2adefc40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Step 1 - Read the JSON file using the spark dataframe reader API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "903f50c6-6811-4e73-b98f-3d69fc2ef886",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f46c76f7-ed21-41ad-8eb2-be7e2794f09f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_schema = StructType(fields=[StructField(\"resultId\", IntegerType(), False),\n",
    "                                    StructField(\"raceId\", IntegerType(), True),\n",
    "                                    StructField(\"driverId\", IntegerType(), True),\n",
    "                                    StructField(\"constructorId\", IntegerType(), True),\n",
    "                                    StructField(\"number\", IntegerType(), True),\n",
    "                                    StructField(\"grid\", IntegerType(), True),\n",
    "                                    StructField(\"position\", IntegerType(), True),\n",
    "                                    StructField(\"positionText\", StringType(), True),\n",
    "                                    StructField(\"positionOrder\", IntegerType(), True),\n",
    "                                    StructField(\"points\", FloatType(), True),\n",
    "                                    StructField(\"laps\", IntegerType(), True),\n",
    "                                    StructField(\"time\", StringType(), True),\n",
    "                                    StructField(\"milliseconds\", IntegerType(), True),\n",
    "                                    StructField(\"fastestLap\", IntegerType(), True),\n",
    "                                    StructField(\"rank\", IntegerType(), True),\n",
    "                                    StructField(\"fastestLapTime\", StringType(), True),\n",
    "                                    StructField(\"fastestLapSpeed\", FloatType(), True),\n",
    "                                    StructField(\"statusId\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaad4053-db78-4a5a-912d-3535537e57ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_df = spark.read.json(\"dbfs:/FileStore/results.json\" , schema = results_schema )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cfdb41c-15b1-4d24-9442-a7c6a8235ca0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Step 2 - Rename columns and add new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3cbc5e1-1efa-4f83-860e-9470b752df60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit , current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7717062f-a0d6-4f4b-8252-2c836c674584",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_with_columns_df = results_df.withColumnRenamed(\"resultId\", \"result_id\") \\\n",
    "                                    .withColumnRenamed(\"raceId\", \"race_id\") \\\n",
    "                                    .withColumnRenamed(\"driverId\", \"driver_id\") \\\n",
    "                                    .withColumnRenamed(\"constructorId\", \"constructor_id\") \\\n",
    "                                    .withColumnRenamed(\"positionText\", \"position_text\") \\\n",
    "                                    .withColumnRenamed(\"positionOrder\", \"position_order\") \\\n",
    "                                    .withColumnRenamed(\"fastestLap\", \"fastest_lap\") \\\n",
    "                                    .withColumnRenamed(\"fastestLapTime\", \"fastest_lap_time\") \\\n",
    "                                    .withColumnRenamed(\"fastestLapSpeed\", \"fastest_lap_speed\") \n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d0786e6-9728-46c3-9c76-a9f7391e7ce6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_with_ingestion_date_df = results_with_columns_df.withColumn(\"ingestion_date\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d05a0b0-27fb-416a-a5a0-1a8b67fe75f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Step 3 - Drop the unwanted column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17d35203-a6e1-46cb-98da-b66d0c56d90e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e97e0fec-cbca-4ed5-be02-8ee156147d4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_final_df = results_with_ingestion_date_df.drop(col(\"statusId\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "292ea247-42c6-4d8a-a364-ac12992b22e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_deduped_df = results_final_df.dropDuplicates(['race_id', 'driver_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65d28820-558e-4eee-8834-88b7151075d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+--------------+------+----+--------+-------------+--------------+------+----+---------+------------+-----------+----+----------------+-----------------+----------------------+\n|result_id|race_id|driver_id|constructor_id|number|grid|position|position_text|position_order|points|laps|time     |milliseconds|fastest_lap|rank|fastest_lap_time|fastest_lap_speed|ingestion_date        |\n+---------+-------+---------+--------------+------+----+--------+-------------+--------------+------+----+---------+------------+-----------+----+----------------+-----------------+----------------------+\n|7573     |1      |1        |1             |1     |18  |null    |D            |20            |0.0   |58  |\\N       |null        |39         |13  |1:29.020        |214.455          |2024-04-23 04:09:58.81|\n|7563     |1      |2        |2             |6     |9   |10      |10           |10            |0.0   |58  |+7.085   |5662869     |48         |5   |1:28.283        |216.245          |2024-04-23 04:09:58.81|\n|7559     |1      |3        |3             |16    |5   |6       |6            |6             |3.0   |58  |+5.722   |5661506     |48         |1   |1:27.706        |217.668          |2024-04-23 04:09:58.81|\n|7558     |1      |4        |4             |7     |10  |5       |5            |5             |4.0   |58  |+4.879   |5660663     |53         |9   |1:28.712        |215.199          |2024-04-23 04:09:58.81|\n|7561     |1      |7        |5             |11    |17  |8       |8            |8             |1.0   |58  |+6.298   |5662082     |50         |17  |1:29.823        |212.537          |2024-04-23 04:09:58.81|\n|7568     |1      |8        |6             |4     |7   |15      |15           |15            |0.0   |55  |\\N       |null        |35         |7   |1:28.488        |215.744          |2024-04-23 04:09:58.81|\n|7567     |1      |9        |2             |5     |4   |14      |14           |14            |0.0   |55  |\\N       |null        |36         |2   |1:27.988        |216.97           |2024-04-23 04:09:58.81|\n|7562     |1      |16       |10            |20    |16  |9       |9            |9             |0.0   |58  |+6.335   |5662119     |43         |11  |1:28.943        |214.64           |2024-04-23 04:09:58.81|\n|7566     |1      |20       |9             |15    |3   |13      |13           |13            |0.0   |56  |\\N       |null        |8          |4   |1:28.140        |216.596          |2024-04-23 04:09:58.81|\n|7555     |1      |22       |23            |23    |2   |2       |2            |2             |8.0   |58  |+0.807   |5656591     |43         |14  |1:29.066        |214.344          |2024-04-23 04:09:58.81|\n|7560     |1      |67       |5             |12    |13  |7       |7            |7             |2.0   |58  |+6.004   |5661788     |34         |16  |1:29.230        |213.95           |2024-04-23 04:09:58.81|\n|7575     |2      |2        |2             |6     |10  |2       |2            |2             |4.0   |31  |+22.722  |4274814     |17         |10  |1:39.084        |201.392          |2024-04-23 04:09:58.81|\n|7581     |2      |3        |3             |16    |4   |8       |8            |8             |0.5   |31  |+1:11.576|4323668     |13         |4   |1:37.598        |204.459          |2024-04-23 04:09:58.81|\n|7593     |2      |5        |1             |2     |14  |null    |R            |20            |0.0   |0   |\\N       |null        |null       |null|\\N              |null             |2024-04-23 04:09:58.81|\n|7592     |2      |9        |2             |5     |6   |null    |R            |19            |0.0   |1   |\\N       |null        |null       |null|\\N              |null             |2024-04-23 04:09:58.81|\n|7577     |2      |15       |7             |9     |2   |4       |4            |4             |2.5   |31  |+46.173  |4298265     |12         |3   |1:37.591        |204.473          |2024-04-23 04:09:58.81|\n|7579     |2      |17       |9             |14    |5   |6       |6            |6             |1.5   |31  |+52.333  |4304425     |14         |5   |1:37.672        |204.304          |2024-04-23 04:09:58.81|\n|7591     |2      |21       |10            |21    |18  |18      |18           |18            |0.0   |29  |\\N       |null        |16         |17  |1:39.407        |200.738          |2024-04-23 04:09:58.81|\n|7578     |2      |22       |23            |23    |8   |5       |5            |5             |2.0   |31  |+47.360  |4299452     |17         |2   |1:37.484        |204.698          |2024-04-23 04:09:58.81|\n|7589     |2      |67       |5             |12    |20  |16      |16           |16            |0.0   |30  |\\N       |null        |16         |8   |1:38.938        |201.689          |2024-04-23 04:09:58.81|\n+---------+-------+---------+--------------+------+----+--------+-------------+--------------+------+----+---------+------------+-----------+----+----------------+-----------------+----------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "results_deduped_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e033b55c-efeb-4e83-a703-dc4ed7f6ad5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Step 4 - Write to output to processed container in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7664adaf-5d56-405c-b2da-1b3bd1a69d5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2747915330769759>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mresults_deduped_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdbfs:/FileStore/Formula1/processed/f1_processed.result\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1655\u001B[0m, in \u001B[0;36mDataFrameWriter.parquet\u001B[0;34m(self, path, mode, partitionBy, compression)\u001B[0m\n",
       "\u001B[1;32m   1653\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpartitionBy(partitionBy)\n",
       "\u001B[1;32m   1654\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(compression\u001B[38;5;241m=\u001B[39mcompression)\n",
       "\u001B[0;32m-> 1655\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Path dbfs:/FileStore/Formula1/processed/f1_processed.result already exists."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2747915330769759>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mresults_deduped_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdbfs:/FileStore/Formula1/processed/f1_processed.result\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1655\u001B[0m, in \u001B[0;36mDataFrameWriter.parquet\u001B[0;34m(self, path, mode, partitionBy, compression)\u001B[0m\n\u001B[1;32m   1653\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpartitionBy(partitionBy)\n\u001B[1;32m   1654\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(compression\u001B[38;5;241m=\u001B[39mcompression)\n\u001B[0;32m-> 1655\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Path dbfs:/FileStore/Formula1/processed/f1_processed.result already exists.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Path dbfs:/FileStore/Formula1/processed/f1_processed.result already exists.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_deduped_df.write.parquet(\"dbfs:/FileStore/Formula1/processed/f1_processed.result\") "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "result processed",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
